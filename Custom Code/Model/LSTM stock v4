class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.hidden_size = hidden_size
        self.query_projections = nn.ModuleList([nn.Linear(hidden_size, hidden_size // num_heads, bias=False) for _ in range(num_heads)])
        self.key_projections = nn.ModuleList([nn.Linear(hidden_size, hidden_size // num_heads, bias=False) for _ in range(num_heads)])
        self.value_projections = nn.ModuleList([nn.Linear(hidden_size, hidden_size // num_heads, bias=False) for _ in range(num_heads)])
        self.output_projection = nn.Linear(hidden_size, hidden_size)
        self.initialize_weights()

    def initialize_weights(self):
        for proj in self.query_projections:
            nn.init.xavier_uniform_(proj.weight)
        for proj in self.key_projections:
            nn.init.xavier_uniform_(proj.weight)
        for proj in self.value_projections:
            nn.init.xavier_uniform_(proj.weight)
        nn.init.xavier_uniform_(self.output_projection.weight)
        if self.output_projection.bias is not None:
            nn.init.zeros_(self.output_projection.bias)

    def forward(self, lstm_output):
        attention_heads = []
        for i in range(self.num_heads):
            queries = self.query_projections[i](lstm_output)
            keys = self.key_projections[i](lstm_output)
            values = self.value_projections[i](lstm_output)
            attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)
            attention_weights = torch.softmax(attention_scores, dim=-1)
            attention_heads.append(torch.matmul(attention_weights, values))
        
        context_vector = torch.cat(attention_heads, dim=-1)
        context_vector = self.output_projection(context_vector)
        return context_vector, attention_weights

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, num_heads=4, dropout_prob=0.5):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.attention = MultiHeadAttention(hidden_size * 2, num_heads)
        self.layer_norm = nn.LayerNorm(hidden_size * 2)
        self.fc = nn.Linear(hidden_size * 2, hidden_size * 2)  # Adjusted to match residual dimensions
        self.output_fc = nn.Linear(hidden_size * 2, output_size)
        self.dropout = nn.Dropout(dropout_prob)
        self.residual_connection = nn.Linear(input_size, hidden_size * 2)

        # Learnable initial states
        self.h_0 = nn.Parameter(torch.zeros(num_layers * 2, 1, hidden_size))
        self.c_0 = nn.Parameter(torch.zeros(num_layers * 2, 1, hidden_size))

        self.initialize_weights()

    def initialize_weights(self):
        for name, param in self.lstm.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                nn.init.zeros_(param.data)
        
        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)
        nn.init.xavier_uniform_(self.output_fc.weight)
        nn.init.zeros_(self.output_fc.bias)
        nn.init.xavier_uniform_(self.residual_connection.weight)
        nn.init.zeros_(self.residual_connection.bias)

    def forward(self, x):
        batch_size = x.size(0)
        h_0 = self.h_0.expand(-1, batch_size, -1).contiguous()
        c_0 = self.c_0.expand(-1, batch_size, -1).contiguous()
        
        lstm_out, _ = self.lstm(x, (h_0, c_0))
        context_vector, attention_weights = self.attention(lstm_out)
        
        context_vector = self.layer_norm(context_vector)
        context_vector = self.dropout(context_vector)
        
        context_vector = self.fc(context_vector)
        
        residual = self.residual_connection(x[:, -1, :])
        residual = residual.unsqueeze(1).expand_as(context_vector)
        
        out = self.output_fc(context_vector + residual)
        out = out[:, -1, :]
        
        return out, attention_weights
